{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187f039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710abd31",
   "metadata": {},
   "source": [
    "# 1) Loading train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a95153",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3e99ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_tokens</th>\n",
       "      <th>punctuation_percentage</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_misspellings</th>\n",
       "      <th>misspelling_percentage</th>\n",
       "      <th>...</th>\n",
       "      <th>emotions</th>\n",
       "      <th>deception_score</th>\n",
       "      <th>money_score</th>\n",
       "      <th>payment_score</th>\n",
       "      <th>celebration_score</th>\n",
       "      <th>achievement_score</th>\n",
       "      <th>url_presence</th>\n",
       "      <th>phone_number_presence</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>pos_verbs_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>calgary , alberta , jul 7 , 2005 ( ccnmatthews...</td>\n",
       "      <td>calgary , alberta , jul 7 , 2005 ( ccnmatthews...</td>\n",
       "      <td>['calgari', 'alberta', 'jul', 'ccnmatthew', 'v...</td>\n",
       "      <td>2.566049</td>\n",
       "      <td>0.028841</td>\n",
       "      <td>0.017150</td>\n",
       "      <td>0.026634</td>\n",
       "      <td>38</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>...</td>\n",
       "      <td>{'help': 0.004336513443191674, 'office': 0.011...</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.01301</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>louise , as of today there is $ 722 , 572 in s...</td>\n",
       "      <td>louise , as of today there is $ 722 , 572 in s...</td>\n",
       "      <td>['louis', 'today', 'schedul', 'c', 'tbg', 'set...</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>8</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>...</td>\n",
       "      <td>{'help': 0.0, 'office': 0.031007751937984496, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.190909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Lmao but its so fun...</td>\n",
       "      <td>Lmao but its so fun...</td>\n",
       "      <td>['lmao', 'fun']</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'mo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>hi : here ' s the presentation . - - - - - ori...</td>\n",
       "      <td>hi : here ' s the presentation . - - - - - ori...</td>\n",
       "      <td>['hi', 'present', 'origin', 'messag', 'kitchen...</td>\n",
       "      <td>8.040712</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>84</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>...</td>\n",
       "      <td>{'help': 0.0, 'office': 0.011441647597254004, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>fraud</td>\n",
       "      <td>fraud</td>\n",
       "      <td>['fraud']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'mo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0  spam  calgary , alberta , jul 7 , 2005 ( ccnmatthews...   \n",
       "1   ham  louise , as of today there is $ 722 , 572 in s...   \n",
       "2   ham                             Lmao but its so fun...   \n",
       "3   ham  hi : here ' s the presentation . - - - - - ori...   \n",
       "4   ham                                              fraud   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  calgary , alberta , jul 7 , 2005 ( ccnmatthews...   \n",
       "1  louise , as of today there is $ 722 , 572 in s...   \n",
       "2                             Lmao but its so fun...   \n",
       "3  hi : here ' s the presentation . - - - - - ori...   \n",
       "4                                              fraud   \n",
       "\n",
       "                                 preprocessed_tokens  punctuation_percentage  \\\n",
       "0  ['calgari', 'alberta', 'jul', 'ccnmatthew', 'v...                2.566049   \n",
       "1  ['louis', 'today', 'schedul', 'c', 'tbg', 'set...                3.571429   \n",
       "2                                    ['lmao', 'fun']               13.636364   \n",
       "3  ['hi', 'present', 'origin', 'messag', 'kitchen...                8.040712   \n",
       "4                                          ['fraud']                0.000000   \n",
       "\n",
       "   num_chars  num_sentences  num_words  num_misspellings  \\\n",
       "0   0.028841       0.017150   0.026634                38   \n",
       "1   0.002575       0.002309   0.002930                 8   \n",
       "2   0.000096       0.000330   0.000160                 2   \n",
       "3   0.008605       0.004947   0.007484                84   \n",
       "4   0.000022       0.000330   0.000027                 0   \n",
       "\n",
       "   misspelling_percentage  ...  \\\n",
       "0                0.038000  ...   \n",
       "1                0.072727  ...   \n",
       "2                0.333333  ...   \n",
       "3                0.298932  ...   \n",
       "4                0.000000  ...   \n",
       "\n",
       "                                            emotions  deception_score  \\\n",
       "0  {'help': 0.004336513443191674, 'office': 0.011...         0.000867   \n",
       "1  {'help': 0.0, 'office': 0.031007751937984496, ...         0.000000   \n",
       "2  {'help': 0.0, 'office': 0.0, 'dance': 0.0, 'mo...         0.000000   \n",
       "3  {'help': 0.0, 'office': 0.011441647597254004, ...         0.000000   \n",
       "4  {'help': 0.0, 'office': 0.0, 'dance': 0.0, 'mo...         1.000000   \n",
       "\n",
       "   money_score payment_score celebration_score  achievement_score  \\\n",
       "0     0.015611       0.01301          0.001735           0.002602   \n",
       "1     0.000000       0.00000          0.000000           0.000000   \n",
       "2     0.000000       0.00000          0.000000           0.000000   \n",
       "3     0.002288       0.00000          0.011442           0.000000   \n",
       "4     0.000000       0.00000          0.000000           0.000000   \n",
       "\n",
       "   url_presence  phone_number_presence  binary_label  pos_verbs_percentage  \n",
       "0             0                      0             1              0.152000  \n",
       "1             0                      0             0              0.190909  \n",
       "2             0                      0             0              0.000000  \n",
       "3             0                      0             0              0.113879  \n",
       "4             0                      0             0              0.000000  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf280f",
   "metadata": {},
   "source": [
    "# 2) Model Training\n",
    "We are using BERT Tiny Uncased model due to computational limitations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e511ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create lists for train and test\n",
    "features = ['num_sentences', 'misspelling_percentage', 'pos_verbs_percentage',\n",
    "             'spaces_percentage', 'sentiment_score', 'money_score', 'payment_score',\n",
    "             'celebration_score', 'achievement_score', 'url_presence',\n",
    "             'phone_number_presence']\n",
    "\n",
    "train_text_data = train_df['cleaned_text'].astype(str).tolist()\n",
    "train_numerical_features = train_df[features].values\n",
    "train_labels = train_df['binary_label'].tolist()\n",
    "\n",
    "test_text_data = test_df['cleaned_text'].astype(str).tolist()\n",
    "test_numerical_features = test_df[features].values\n",
    "test_labels = test_df['binary_label'].tolist()\n",
    "\n",
    "# Step 2: Tokenization\n",
    "max_seq_length = 128\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "train_tokenized_texts = [tokenizer.tokenize(text)[:max_seq_length] for text in train_text_data]  \n",
    "test_tokenized_texts = [tokenizer.tokenize(text)[:max_seq_length] for text in test_text_data]  \n",
    "\n",
    "train_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in train_tokenized_texts]\n",
    "test_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in test_tokenized_texts]\n",
    "\n",
    "# Step 3: Padding\n",
    "train_input_ids = torch.tensor([ids + [0]*(max_seq_length-len(ids)) for ids in train_input_ids], dtype=torch.long)\n",
    "test_input_ids = torch.tensor([ids + [0]*(max_seq_length-len(ids)) for ids in test_input_ids], dtype=torch.long)\n",
    "\n",
    "# Step 4: Combine Text and Numerical Features for Train and Test Data\n",
    "train_numerical_features_array = np.array(train_numerical_features, dtype=np.float32)\n",
    "train_numerical_features_tensor = torch.tensor(train_numerical_features_array)\n",
    "\n",
    "test_numerical_features_array = np.array(test_numerical_features, dtype=np.float32)\n",
    "test_numerical_features_tensor = torch.tensor(test_numerical_features_array)\n",
    "\n",
    "train_combined_features = torch.cat([train_input_ids, train_numerical_features_tensor], dim=1)\n",
    "test_combined_features = torch.cat([test_input_ids, test_numerical_features_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bf271",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d84954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Loading and adjusting model parameters\n",
    "\n",
    "#Parameters\n",
    "lr = 2e-5\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "#Model and Optimizer\n",
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Example optimizer, adjust as needed\n",
    "train_data = TensorDataset(train_combined_features, torch.tensor(train_labels, dtype=torch.long))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64790d07",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71eecf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Model training\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs.long(), labels=batch_labels)  # Convert input tensor to long tensor\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c9106",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Despite only training for one epoch, the F1-score for both spam and ham texts is relatively high. We will proceed to find out how we can improve the current model through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b0f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      7169\n",
      "           1       0.95      0.92      0.93      4704\n",
      "\n",
      "    accuracy                           0.95     11873\n",
      "   macro avg       0.95      0.94      0.95     11873\n",
      "weighted avg       0.95      0.95      0.95     11873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Model evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(test_combined_features.long()).logits  # Convert input tensor to long tensor\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    predictions = predictions.numpy()\n",
    "    report = classification_report(test_labels, predictions)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297fc632",
   "metadata": {},
   "source": [
    "# 3) Hyperparameter tuning\n",
    "We split up the different optimizers due to computational constraints, we will then evaluate the performance of the best hyperparameters for each optimzer and choose the best performing model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b147e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifierWrapper(BaseEstimator):\n",
    "    def __init__(self, num_numerical_features, num_labels=2, optimizer='AdamW', lr=2e-5, num_epochs=3, batch_size=32):\n",
    "        self.num_numerical_features = num_numerical_features\n",
    "        self.num_labels = num_labels\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize BERT model\n",
    "        self.bert_model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=num_labels)\n",
    "        \n",
    "        # Define additional layers for processing numerical features\n",
    "        self.linear = torch.nn.Linear(num_numerical_features, 128)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.final_linear = torch.nn.Linear(128, num_labels)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert input data to tensors\n",
    "        numerical_features = torch.tensor(X[:, :self.num_numerical_features], dtype=torch.float32)\n",
    "        text_features = [str(text) for text in X[:, self.num_numerical_features:]]\n",
    "        labels = torch.tensor(y, dtype=torch.long)\n",
    "        tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "        tokenized_texts = [tokenizer.tokenize(text)[:128] for text in text_features]  # Tokenize text-based features\n",
    "        input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]\n",
    "        input_ids = torch.tensor([ids + [0]*(128-len(ids)) for ids in input_ids], dtype=torch.long)  # Pad tokenized inputs\n",
    "\n",
    "        # Train the model\n",
    "        self.bert_model.train()\n",
    "        if self.optimizer == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW([\n",
    "                {'params': self.bert_model.parameters()},\n",
    "                {'params': self.linear.parameters()},\n",
    "                {'params': self.final_linear.parameters()}\n",
    "            ], lr=self.lr)\n",
    "        elif self.optimizer == 'SGD':\n",
    "            optimizer = torch.optim.SGD([\n",
    "                {'params': self.bert_model.parameters()},\n",
    "                {'params': self.linear.parameters()},\n",
    "                {'params': self.final_linear.parameters()}\n",
    "            ], lr=self.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        num_batches = len(input_ids) // self.batch_size\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i in range(num_batches):\n",
    "                batch_input_ids = input_ids[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                batch_numerical_features = numerical_features[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                batch_labels = labels[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.bert_model(input_ids=batch_input_ids)  # Pass tokenized inputs to BERT model\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Additional layers for numerical features\n",
    "                x = self.linear(batch_numerical_features)\n",
    "                x = self.activation(x)\n",
    "                x = self.final_linear(x)\n",
    "\n",
    "                loss = torch.nn.CrossEntropyLoss()(logits, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert input data to tensors\n",
    "        numerical_features = torch.tensor(X[:, :self.num_numerical_features], dtype=torch.float32)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        self.bert_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(inputs)  # Assuming inputs are already tokenized\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Additional layers for numerical features\n",
    "            x = self.linear(numerical_features)\n",
    "            x = self.activation(x)\n",
    "            x = self.final_linear(x)\n",
    "            \n",
    "            predictions = torch.argmax(x, dim=1)\n",
    "        return predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f789d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'batch_size': 32, 'lr': 0.002, 'num_epochs': 1, 'optimizer': 'AdamW'}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'optimizer': ['AdamW'], #, 'SGD'\n",
    "    'lr': [2e-3, 2e-1],  # Learning rates to tune\n",
    "    'num_epochs': [1],  # Number of epochs to tune\n",
    "    'batch_size': [32, 16]\n",
    "}\n",
    "\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=BertClassifierWrapper(num_numerical_features=11, num_labels=2), \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='f1',  # Choose appropriate scoring metric\n",
    "                           cv=2)  # Number of cross-validation folds\n",
    "\n",
    "# Fit the GridSearchCV instance on the training data\n",
    "grid_search.fit(train_combined_features, train_labels)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543aaf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'batch_size': 32, 'lr': 0.002, 'num_epochs': 1, 'optimizer': 'SGD'}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'optimizer': ['SGD'], #, 'SGD'\n",
    "    'lr': [2e-3, 2e-1],  # Learning rates to tune\n",
    "    'num_epochs': [1],  # Number of epochs to tune\n",
    "    'batch_size': [32, 16]\n",
    "}\n",
    "\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=BertClassifierWrapper(num_numerical_features=11, num_labels=2), \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='f1',  # Choose appropriate scoring metric\n",
    "                           cv=2)  # Number of cross-validation folds\n",
    "\n",
    "# Fit the GridSearchCV instance on the training data\n",
    "grid_search.fit(train_combined_features, train_labels)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7392ae",
   "metadata": {},
   "source": [
    "# Testing best model for each optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d941a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "lr = 2e-3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16db543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (Adam): \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92      7169\n",
      "           1       0.93      0.81      0.87      4704\n",
      "\n",
      "    accuracy                           0.90     11873\n",
      "   macro avg       0.91      0.89      0.90     11873\n",
      "weighted avg       0.91      0.90      0.90     11873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model 1 Adam Optimiser\n",
    "model1 = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=lr)  # Example optimizer, adjust as needed\n",
    "train_data = TensorDataset(train_combined_features, torch.tensor(train_labels, dtype=torch.long))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "model1.train()\n",
    "for epoch in range(1):\n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model1(batch_inputs.long(), labels=batch_labels)  # Convert input tensor to long tensor\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model1(test_combined_features.long()).logits  # Convert input tensor to long tensor\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    predictions = predictions.numpy()\n",
    "    report = classification_report(test_labels, predictions)\n",
    "    print('Model 1 (Adam): \\n',report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67e1abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 (SGD): \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90      7169\n",
      "           1       0.81      0.93      0.86      4704\n",
      "\n",
      "    accuracy                           0.88     11873\n",
      "   macro avg       0.88      0.89      0.88     11873\n",
      "weighted avg       0.89      0.88      0.88     11873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 2 SGD Optimiser    \n",
    "model2 = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=lr)  # Example optimizer, adjust as needed\n",
    "train_data = TensorDataset(train_combined_features, torch.tensor(train_labels, dtype=torch.long))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model2.train()\n",
    "for epoch in range(1):\n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(batch_inputs.long(), labels=batch_labels)  # Convert input tensor to long tensor\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model2(test_combined_features.long()).logits  # Convert input tensor to long tensor\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    predictions = predictions.numpy()\n",
    "    report = classification_report(test_labels, predictions)\n",
    "    print('Model 2 (SGD): \\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04ae29",
   "metadata": {},
   "source": [
    "# Final Model Training\n",
    "We will use the chosen model to undergo more epochs (from 1 to 3) during training so as to achieve a better performing model. Most open source forums discuss that fine tuning pre-existing BERT models should not exceed 5 epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0549d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85      7169\n",
      "           1       0.89      0.53      0.67      4704\n",
      "\n",
      "    accuracy                           0.79     11873\n",
      "   macro avg       0.82      0.74      0.76     11873\n",
      "weighted avg       0.81      0.79      0.77     11873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_loss_values = []\n",
    "\n",
    "final_model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=lr) \n",
    "train_data = TensorDataset(train_combined_features, torch.tensor(train_labels, dtype=torch.long))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(6):\n",
    "    epoch_loss = 0.0  \n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_inputs.long(), labels=batch_labels)  # Convert input tensor to long tensor\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    training_loss_values.append(epoch_loss)\n",
    "\n",
    "# Evaluation\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = final_model(test_combined_features.long()).logits  # Convert input tensor to long tensor\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    predictions = predictions.numpy()\n",
    "    report = classification_report(test_labels, predictions)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3134e249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss\n",
    "plt.plot(training_loss_values, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b73412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87      7169\n",
      "           1       0.89      0.67      0.76      4704\n",
      "\n",
      "    accuracy                           0.83     11873\n",
      "   macro avg       0.85      0.81      0.82     11873\n",
      "weighted avg       0.84      0.83      0.83     11873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=lr) \n",
    "train_data = TensorDataset(train_combined_features, torch.tensor(train_labels, dtype=torch.long))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(2):\n",
    "    epoch_loss = 0.0  \n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_inputs.long(), labels=batch_labels)  # Convert input tensor to long tensor\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = final_model(test_combined_features.long()).logits  # Convert input tensor to long tensor\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    predictions = predictions.numpy()\n",
    "    report = classification_report(test_labels, predictions)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ddb62",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea465e",
   "metadata": {},
   "source": [
    "Despite lower training loss, the model trained using an additional epoch results in poorer performance. Hence, we will adopt the previous model which is trained using 1 epoch using the Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52ea49d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "torch.save(model1.state_dict(), 'bert_model.pth')\n",
    "\n",
    "# Loading the model\n",
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "model.load_state_dict(torch.load('bert_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db8a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "state_dict = torch.load('bert_model.pth')\n",
    "state_dict.pop('bert.embeddings.position_ids')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Examples\n",
      "Text: who is a close frnd? woh jo pehla aasu tham le phir aansu poch de or phir chehre ko haanthon me tham kar kahe chal drame band kar !! :d\n",
      "Text: Same here, but I consider walls and bunkers and shit important just because I never play on peaceful but I guess your place is high enough that it don't matter\n",
      "Text: hello did any of you hear about a a razor plugin for mozilla a plugin that would add a report as spam button somewhere for the user to report a mail as spam on a server arnaud arnaud ablard administrateur rseaux et systmes irin facult de sciences universit de nantes this url email is sponsored by osdn tired of that same old cell phone get a new here for free url razor users mailing list razor users url url\n",
      "Text: in sigh here is esai ' s latest natural gas fundwatch . edna o ' connell office manager esai 301 edgewater place , suite 108 wakefield , ma 01880 ( 781 ) 245 - 2036 ( 781 ) 245 - 8706 ednao @ esaibos . com - ngol 1900 . pdf\n",
      "Text: 20210406 204127  600\n",
      "Text: great i hope you like your man well endowed i am ltgt inches\n",
      "Text: no we sell it all so well have tons if coins then sell our coins to someone thru paypal voila money back in life pockets\n",
      "Text: here are the days i have discrepancies with pg & e . please let me know which numbers are good . thanks thu 3 - 7159 day pg & e hpl 12 - 05 145833 120000 12 - 10 60000 35000 12 - 11 70000 80000 12 - 17 3125 3100 12 - 21 50000 70000 12 - 22 0 20000 12 - 25 0 30000 12 - 26 0 30000 12 - 27 31375 63000\n",
      "Text: hey galsu all wanna meet 4 dinner at nte\n",
      "Text: don ' t save passwords internet explorer and some web based applications give you the ability to save your password , so you don ' t have to type it in each time you access it . unfortunately , this time - saver can also get you in trouble when you are forced to change your password and the new password doesn ' t get saved . we ' ve had numerous calls to the help desk due to old passwords still being \" saved \" . to keep yourself from being a victim ( and for good security ) , please do not save passwords when prompted . some examples are below . . . don ' t update software or install plug - ins most of us have gone to a web site that prompted us to update our software version or shows a link to install a new plug - in so we can see all the fancy web page components . many vendor web sites let you download demos of their software . we need to resist the temptation to do this , since it may not be approved software and could cause applications to malfunction . this can further lead to difficulty in tracking down the cause of problems , since the workstation is now \" non - standard \" . only the omaha it staff should be installing software . when in doubt , please contact the help desk for assistance . examples from web sites are below . . . . thanks ! : _ _ _ _ _ _ _ tyler a theobald ets omaha it 402 - 398 - 7247 solution center : 402 - 398 - 7454 \n",
      "\n",
      "\n",
      "False Negative Examples\n",
      "Text:  hey there !! welcome to our new telegram channel sosstudies here you will be given all study materials needed to excel in your studies this channel contains all the required science notes for class x  plz subscribe and share this channel thank you!!\n",
      "Text: congratulations on receiving this special e - mail invitation ! ! ! these invitations were only being sent out . . . to a very select group of individuals like yourself . . . who were confirmed and qualified to receive this spectacular offer ! ! ! please click on the link below to register and receive your complimentary three night stay in your choice of three ( 3 ) of these nine ( 9 ) fun filled locations ! ! ! - magical orlando - las vegas . . . city of lights - palm beach , fl . . . . florida ' s best kept secret - fabulous ft , lauderdale - atlantic city . . . city of excitement - new smyrna beach , fl . . . . secluded from it all - daytona beach , fl . . . the worlds most famous beach - key west , fl . . . the southern most point in the u . s . - miami south beach . the city that never sleeps so log onto : http : / / www . famtriptravel . com / 3 mv _ cntr . html . . . for your complimentary vacations for two ! ! ! keep in mind . . . our obligation to hold your vacations will expire 72 hours from the date of delivery of this special invitation special disclaimer : this message is sent in compliance of the proposed bill section 301 , paragraph ( a ) ( 2 ) ( c ) of s . 1618 . by providing a valid remove me feature it can not be considered spam . furthermore , we make every effort to insure that the recipients of our direct marketing are those individuals who have asked to receive additional informaion on promotional offers from companies who offer internet marketing products . again we apologize if this message has reached you in error . screening of addresses has been done to the best of our technical ability . we honor all removal requests . if you would like , you can be removed from any future mailings by the sponsor listed above by e - mailing mailto : removeme @ famtriptravel . com with the subject remove me in the subject line . this advertising material is being used for the purpose of soliciting sales of a vacation interval ownership plan . \n",
      "Text: your family could definately use this , now go . gqxoayoh\n",
      "Text: hello , i just found a site called graand . com - a free and safe place on the internet to place classified ads . i thought i should invite you to check it out . regards , walkerczesc , wlasnie znalazlam stronke w internecie graand . com - miejsce w internecie , gdzie mozesz dawac darmowe ogloszenia . pomyslalam , ze cie to zainteresuje . pozdrawiam , walker\n",
      "Text: You have won ?1,000 cash or a ?2,000 prize! To claim, call09050000327. T&C: RSTM, SW7 3SS. 150ppm\n",
      "Text:                                         reborncrewbot               :  200          drop username and name if want to participate\n",
      "Text: attn . i am , ( head of group risk ) bankers trust international london , england . between 31 st july , 1999 / jan . 12 th 2000 , one mr . andreas schranner , a german national , a property magnate , made series of ( fixed ) deposit , valued at # 2 , 550 , 000 . 00 ( two million , five hundred and fifty thousand pounds ) in my bank branch , upon maturity and as would be required , we sent a routine notification to his forwarding address but got no response . after a month , we became apprehensive and sent a reminder . finally we discovered from his company that mr . andreas schranner was aboard the ill fated af 4590 plane , which crashed monday , 31 july , 2000 into the hotelissimo . after further investigation , it was discovered that he died without making a will and all attempts to trace his next of kin proved abortive and futile . as a matter of fact and on further investigation , it was discovered that late mr . andreas schranner did not declare any next of kin or relatives in all his official documents , including his bank deposit paper work here in our bank . this fund is still in my bank and the interest is being rolled over with the principal sum at the end of each year . no one will ever and has come forward to lay claim to it . in strict accordance with the british banking laws and constitution , at the expiration of 5 ( five ) years , the money will revert to the british government treasury if nobody applies as the next of kin to claim the funds . all i need from you is to play the role as next of kin , the bank will work with verifiable documents which i have . upon acceptance of this proposal , i shall send to you the bankers trust international banks next of kin payment application form as well as detailed information on how this transaction would be carried out . we shall employ the services of a solicitor for the drafting of the last will testament of late mr . andreas schranner and to obtain all other relevant papers in your name for the necessary documentation for payment approval in your favor . i did my homework thoroughly and very well before contacting you . i am open for negotiation on your terms . i guarantee that this will be executed under a legitimate arrangement that will protect you from any breach of the law as i will use my official position and capacity to secure approvals and guarantee the successful execution of this transaction . please be informed that your utmost confidentiality is required . if this interests you , please , indicate by responding with your contact details . thanks . best regards , p . hall rickhall @ web - mail . com . ar mail enviado desde el servicio de webmail de audinfor site - http : / / audinfor . com \n",
      "Text: my dear , i have a profiling amount in an excess of us $ 423 m , which i seek your partnership in accommodating for me . you will be rewarded with 40 % of the total sum for your partnership . can you be my partner on this ? introduction of my self : i am ms . kimaeva lioudmila , a personal secretary to mikhail khodorkovskythe richest man in russia and owner of the following companies : chairman ceo : yukos oil ( russian most largest oil company ) chairman ceo : menatep sbp bank ( a well reputable financial institution with its branchesall over the world ) source of funds : the documents of the above funds in question was handed over to me tobe used in payment of an american oil merchant for his last oil dealwith my boss mikhail khodorkovsky . already the funds have been depositedwith tresury services europe , where the final crediting is expected to be carried out . while i was on the process , my boss got arrested forhis involvement on politics in financing the leading and opposing political parties ( the union of right forces , led by boris nemtsov , and yabloko , a liberal / social democratic party led by gregor yavlinsky ) which poses treat topresident vladimir putin second tenure as russian president . you can catch more of the story on this your role : all i need from you is to stand as the beneficiary of the above quoted sum and i will arrange for the documentation which will enable tresury services europe transfer the sum to you . i have decided to use this sum to relocate to american continent and never to be connected to any of mikhail khodorkovsky conglomerates . the transaction has to be concluded in 2 weeks before mikhail khodorkovsky is out on bail . as soon as i get your willingness to comply through my most private email address ( mila 2007 @ tsamail . co . za ) i will give you more details . thank you very much regards kimaeva lioudmila\n",
      "Text: name - brand software at low , low , low , low prices the art of life lies in a constant readjustment to our surroundings . as a man thinks in his heart , so is he .\n",
      "Text: this is our absolute attempt we have tried to speak to you on quite a few possibilities and we hope for you reply this time ! your current home loan certifies you for up to a 3 . 40 % lower rate . however , based on the fact that our previous attempts to speak to you were unsuccessful , this will be our last try to finalize the lower rate . please bring to an end this final step upon receiving this notice immediately , and complete your request for information now . application here . if your decision is not to make use of this final offer going here will help you to do so .\n"
     ]
    }
   ],
   "source": [
    "logits = model(test_combined_features.long()).logits  # Convert input tensor to long tensor\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "# Initialize counters for false positives and false negatives\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "max_count = 10\n",
    "\n",
    "# Iterate over test examples for false positives\n",
    "print(\"False Positive Examples\")\n",
    "for idx in range(len(predictions)):\n",
    "    # Check if the example is a false positive\n",
    "    if predictions[idx] == 1 and test_labels[idx] == 0 and fp_count < max_count:\n",
    "        print(\"Text:\", test_text_data[idx])\n",
    "        fp_count += 1\n",
    "    \n",
    "    # Break loop if we have found 5 examples\n",
    "    if fp_count >= max_count:\n",
    "        break\n",
    "\n",
    "# Iterate over test examples for false negatives\n",
    "print(\"\\n\\nFalse Negative Examples\")\n",
    "for idx in range(len(predictions)):\n",
    "    # Check if the example is a false negative\n",
    "    if predictions[idx] == 0 and test_labels[idx] == 1 and fn_count < max_count:\n",
    "        print(\"Text:\", test_text_data[idx])\n",
    "        fn_count += 1\n",
    "    \n",
    "    # Break loop if we have found 5 examples\n",
    "    if fn_count >= max_count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1d445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
